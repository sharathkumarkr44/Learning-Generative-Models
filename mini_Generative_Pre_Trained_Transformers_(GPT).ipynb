{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Referred to:- https://deepkondah.medium.com/step-by-step-implementation-of-generative-pre-trained-transformers-gpt-3c8e09622645"
      ],
      "metadata": {
        "id": "TPdsURDMA0wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install keras_nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwvEY8N-c6lK",
        "outputId": "d12eed69-4d5c-4bef-a2cb-d95b0613bae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: keras_nlp in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (24.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2024.5.15)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.2.5)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (4.66.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.16.1)\n",
            "Requirement already satisfied: tensorflow<2.17,>=2.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.37.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2024.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.43.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.11.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArIgphOEWX2B",
        "outputId": "0bc168fd-b020-4c55-89f0-34478b0602c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_nlp in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (24.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2024.5.15)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.2.5)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (4.66.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.16.1)\n",
            "Requirement already satisfied: tensorflow<2.17,>=2.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.37.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2024.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.43.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.11.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import Concatenate, TextVectorization\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizerFast\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "id": "8gPKOZiScxSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AttentionHead class processes the input by:\n",
        "\n",
        "    Mapping the input to query, key, and value spaces.\n",
        "    Computing scaled dot-product attention, including masking to ensure causal attention.\n",
        "    Applying softmax to get the attention weights.\n",
        "    Using the attention weights to compute a weighted sum of the value vectors."
      ],
      "metadata": {
        "id": "EYmIwZeVo2RV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wICon9mhFl9"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(layers.Layer):\n",
        "\n",
        "    def __init__(self, embedding_space_dimension):\n",
        "        super().__init__()\n",
        "        self.q_mapping = layers.Dense(embedding_space_dimension)\n",
        "        self.k_mapping = layers.Dense(embedding_space_dimension)\n",
        "        self.v_mapping = layers.Dense(embedding_space_dimension)\n",
        "\n",
        "    def call(self, x):\n",
        "        q = self.q_mapping(x)\n",
        "        v = self.v_mapping(x)\n",
        "        k = self.k_mapping(x)\n",
        "        return self.scaled_dot_product_attention(q, k, v)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        w = tf.matmul(q, k, transpose_b=True)\n",
        "        d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        w = w / tf.sqrt(d_k)\n",
        "        w = self.mask_attn_weights(w)\n",
        "        w = tf.nn.softmax(w)\n",
        "        o = tf.matmul(w, v)\n",
        "        return o\n",
        "\n",
        "    def mask_attn_weights(self, w):\n",
        "        shape = tf.shape(w)\n",
        "        n = shape[1]\n",
        "        attention_mask = self.attention_mask(n, w.dtype)\n",
        "        attention_mask = tf.reshape(attention_mask, [1, n, n])\n",
        "        m = tf.reshape(attention_mask, [1, n, n])\n",
        "        w = w * m - tf.cast(1e11, w.dtype) * (1 - m)\n",
        "        return w\n",
        "\n",
        "    def attention_mask(self, n, dtype):\n",
        "        \"\"\"\n",
        "        1's positioned in the lower triangular part, starting from the bottom-right corner.\n",
        "        example:\n",
        "          M =  [ 1 0 0\n",
        "                 1 1 0\n",
        "                 1 1 1]\n",
        "        \"\"\"\n",
        "        i = tf.range(n)[:, None]\n",
        "        j = tf.range(n)\n",
        "        m = i >= j\n",
        "        return tf.cast(m, dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Attention Heads Process the Input: Each attention head independently computes its own version of attention.\n",
        "\n",
        "Concatenate Outputs: The outputs from all attention heads are combined into a single tensor.\n",
        "\n",
        "Linear Projection: This combined tensor is passed through a dense layer to get back to the original embedding dimension."
      ],
      "metadata": {
        "id": "C98mX9q7yPwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiAttentionHead(layers.Layer):\n",
        "\n",
        "    def __init__(self, embedding_space_dimension, numb_heads):\n",
        "        super().__init__()\n",
        "        self.attention_heads = [AttentionHead(embedding_space_dimension) for _ in range(numb_heads)]\n",
        "        self.linear = layers.Dense(embedding_space_dimension)\n",
        "\n",
        "    def call(self, x):\n",
        "        heads = Concatenate()([attention_head(x) for attention_head in self.attention_heads])\n",
        "        return self.linear(heads)"
      ],
      "metadata": {
        "id": "LDqSrJpYo3lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Attention:\n",
        "\n",
        "    The input is processed by a multi-head self-attention mechanism.\n",
        "    Dropout is applied to the attention output.\n",
        "    The result is added to the original input (residual connection).\n",
        "    Layer normalization is applied to the sum.\n",
        "\n",
        "Feedforward Network:\n",
        "\n",
        "    The result from the self-attention block is processed by a feedforward network.\n",
        "    Dropout is applied to the FFN output.\n",
        "    The result is added to the input of the FFN block (another residual connection).\n",
        "    Layer normalization is applied to this sum."
      ],
      "metadata": {
        "id": "3nQ4IE3DPSIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, embedding_space_dimension, numb_heads, ffn_dimension):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiAttentionHead(embedding_space_dimension, numb_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ffn_dimension, activation=\"relu\"), layers.Dense(embedding_space_dimension), ])\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.norm1(x + self.dropout1(self.self_attention(x)))\n",
        "        x = self.norm2(x + self.dropout2(self.ffn(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "r5QDTaF1yvP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converts the token indices to token embeddings.\n",
        "\n",
        "Converts the position indices to positional embeddings.\n",
        "\n",
        "Adds the token embeddings and positional embeddings to produce the final combined embeddings."
      ],
      "metadata": {
        "id": "akiZXFcWTMwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "jYMEAcHbyvZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed the Input:\n",
        "\n",
        "    Convert token indices to dense vectors and add positional information.\n",
        "\n",
        "Process with Transformer Decoder Blocks: (by default 1)\n",
        "\n",
        "    Pass the embeddings through several transformer decoder blocks to capture complex patterns in the sequence.\n",
        "\n",
        "Generate Predictions:\n",
        "\n",
        "    Project the processed embeddings to the vocabulary size to obtain logits for each token position."
      ],
      "metadata": {
        "id": "_i87ImDQWMa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(layers.Layer):\n",
        "    def __init__(self, maxlen, embedding_space_dimension, numb_heads, vocab_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.transformer_decoder_blocks = keras.Sequential([TransformerDecoderBlock(\n",
        "            embedding_space_dimension=embedding_space_dimension,\n",
        "            numb_heads=numb_heads,\n",
        "            ffn_dimension=embedding_space_dimension,\n",
        "        ) for _ in range(num_layers)])\n",
        "        self.input_embedding = TokenAndPositionEmbedding(maxlen, vocab_size, embedding_space_dimension)\n",
        "        self.prediction_output = keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.input_embedding(x)\n",
        "        x = self.transformer_decoder_blocks(x)\n",
        "        o = self.prediction_output(x)\n",
        "        return o"
      ],
      "metadata": {
        "id": "582YwGryyvcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 121\n",
        "projection_dimension = 256\n",
        "n_heads = 8\n",
        "vocab_size = 30522\n",
        "nb_layers = 2\n",
        "\n",
        "gpt = GPT(maxlen, projection_dimension, n_heads, vocab_size, nb_layers)\n",
        "inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "model = keras.Model(name=\"gpt\", inputs=inputs, outputs=gpt(inputs))\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True)\n",
        "model.compile(\"adam\", loss=loss_fn)\n",
        "\n",
        "print(\"GPT model compiled successfully\")\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "Y5VG8AyQWc0L",
        "outputId": "f5a4fc8a-03ec-4a6a-a8f7-46e775163d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT model compiled successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gpt\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gpt_1 (\u001b[38;5;33mGPT\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m30522\u001b[0m)          │      \u001b[38;5;34m20,161,082\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gpt_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30522</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">20,161,082</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,161,082\u001b[0m (76.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,161,082</span> (76.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,161,082\u001b[0m (76.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,161,082</span> (76.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnel4PQmWwWE",
        "outputId": "61827c10-d8cf-487a-d948-4dae67477965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  12.5M      0  0:00:06  0:00:06 --:--:-- 11.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = '[start] ' + text + ' [end]'\n",
        "    return text\n",
        "\n",
        "dirs = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/pos\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "\n",
        "def read_files(dirs):\n",
        "    texts = []\n",
        "    for dir in dirs:\n",
        "        for file_name in os.listdir(dir):\n",
        "            file_path = os.path.join(dir, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                preprocessed_text = preprocess(text)\n",
        "                texts.append(preprocessed_text)\n",
        "    return texts\n",
        "\n",
        "texts = read_files(dirs)\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "maxlen = 121\n",
        "\n",
        "encoded_texts = tokenizer(texts, truncation=True, padding='max_length', max_length=maxlen + 1, return_tensors='np')\n",
        "\n",
        "# Extract input_ids from the encoded texts\n",
        "input_ids = encoded_texts['input_ids']\n",
        "\n",
        "# Prepare input and output sequences\n",
        "final_dataset = input_ids\n",
        "inputs = final_dataset[:, :-1]\n",
        "outputs = final_dataset[:, 1:]\n",
        "\n",
        "print(\"Inputs:\", inputs[:2])\n",
        "print(\"Outputs:\", outputs[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRmuZaKNaxT5",
        "outputId": "bcbc108a-b9c0-49cd-b14c-9fd35f47fc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: [[  101  1031  2707  1033  2017  3726  2196  2464  2505  2066  2009  2320\n",
            "   1996  8648  4269  2049  1996  2087 28190  3341 11253 29337 22573  4017\n",
            "  10874  2017  3363  2412  2156  2130  2295  2017  2113  1996  9560  1998\n",
            "   2049  2035  2613  2138  2049  1037  4516  6429  7987  7987  2011  1996\n",
            "   2051  2009  2001  2058  2009  2001  2006  2026  2327  2184  2862  1997\n",
            "   2035  2051  2307  5691 19892  7987 27770  1996 22889 16429  5677  2075\n",
            "   2157  9328  5470 17592  3071  1045  2113  2040  2038  2464  2023  2143\n",
            "   3957  2009  1996  1018 14117  5790  2130  2065  2017  2123  2102  2729\n",
            "   2055  4331  2030  2055 15332  4331  2017  2097  2424  4426  9113 22648\n",
            "   8126  1998  2903  2009  2006  1996  3341  1997  2115  2835 19892  7987\n",
            "   2049]\n",
            " [  101  1031  2707  1033 12807  1998  3811  6383 12127  3235  8040  5668\n",
            "   6810 20798  1037  4989  1997  2010  5440  2137  3152 15131  2429  2000\n",
            "   2093  2367  4127  1997  5501  1996  2472  2004  2019 12492  2923  1040\n",
            "   2860 14135  2030  1042  1059 14163 12789  2226  2040  2580  2047  9260\n",
            "   5461  2426  2060  3431  2008  2081  1996  3311  1997  2614  1998  3609\n",
            "   2101  3357  2830  1996  2472  2004  1037 20673 17420 16587  2107  2004\n",
            "   5203  2909  2243  5212 12548  1998  3262  6320  2063  8117  9091  2072\n",
            "   5501  2040  2109  2000 14249 22614  7696  1999  2037  3152  1998  1996\n",
            "   2472  2004 12696 10085  8523  2102  2216 16587  7866  2942  9420  1998\n",
            "   2591  6865 22264  2066 25026 23447 17513  3854  2358  3217  8049  2798\n",
            "  23331]]\n",
            "Outputs: [[ 1031  2707  1033  2017  3726  2196  2464  2505  2066  2009  2320  1996\n",
            "   8648  4269  2049  1996  2087 28190  3341 11253 29337 22573  4017 10874\n",
            "   2017  3363  2412  2156  2130  2295  2017  2113  1996  9560  1998  2049\n",
            "   2035  2613  2138  2049  1037  4516  6429  7987  7987  2011  1996  2051\n",
            "   2009  2001  2058  2009  2001  2006  2026  2327  2184  2862  1997  2035\n",
            "   2051  2307  5691 19892  7987 27770  1996 22889 16429  5677  2075  2157\n",
            "   9328  5470 17592  3071  1045  2113  2040  2038  2464  2023  2143  3957\n",
            "   2009  1996  1018 14117  5790  2130  2065  2017  2123  2102  2729  2055\n",
            "   4331  2030  2055 15332  4331  2017  2097  2424  4426  9113 22648  8126\n",
            "   1998  2903  2009  2006  1996  3341  1997  2115  2835 19892  7987  2049\n",
            "    102]\n",
            " [ 1031  2707  1033 12807  1998  3811  6383 12127  3235  8040  5668  6810\n",
            "  20798  1037  4989  1997  2010  5440  2137  3152 15131  2429  2000  2093\n",
            "   2367  4127  1997  5501  1996  2472  2004  2019 12492  2923  1040  2860\n",
            "  14135  2030  1042  1059 14163 12789  2226  2040  2580  2047  9260  5461\n",
            "   2426  2060  3431  2008  2081  1996  3311  1997  2614  1998  3609  2101\n",
            "   3357  2830  1996  2472  2004  1037 20673 17420 16587  2107  2004  5203\n",
            "   2909  2243  5212 12548  1998  3262  6320  2063  8117  9091  2072  5501\n",
            "   2040  2109  2000 14249 22614  7696  1999  2037  3152  1998  1996  2472\n",
            "   2004 12696 10085  8523  2102  2216 16587  7866  2942  9420  1998  2591\n",
            "   6865 22264  2066 25026 23447 17513  3854  2358  3217  8049  2798 23331\n",
            "    102]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                         patience=3)\n",
        "\n",
        "model.fit(x=inputs, y=outputs, epochs=15, callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT1E7Olaa7e9",
        "outputId": "9b5af3c4-e3eb-409c-b064-8a7436c27949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 179ms/step - loss: 5.8257\n",
            "Epoch 2/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 131ms/step - loss: 4.6363\n",
            "Epoch 3/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 130ms/step - loss: 4.3659\n",
            "Epoch 4/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 131ms/step - loss: 4.1921\n",
            "Epoch 5/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 131ms/step - loss: 4.0628\n",
            "Epoch 6/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 130ms/step - loss: 3.9641\n",
            "Epoch 7/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 131ms/step - loss: 3.8690\n",
            "Epoch 8/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 131ms/step - loss: 3.7932\n",
            "Epoch 9/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 128ms/step - loss: 3.7333\n",
            "Epoch 10/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 130ms/step - loss: 3.6636\n",
            "Epoch 11/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 131ms/step - loss: 3.6052\n",
            "Epoch 12/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 131ms/step - loss: 3.5607\n",
            "Epoch 13/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 131ms/step - loss: 3.5194\n",
            "Epoch 14/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 132ms/step - loss: 3.4775\n",
            "Epoch 15/15\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 131ms/step - loss: 3.4453\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f1b1610dde0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to generate text one by one\n",
        "def generate_text(model, tokenizer, input_text, max_length, sampler):\n",
        "    input_padded_tokens = tokenizer(text, truncation=True, padding='max_length', max_length=max_length)\n",
        "    input_token_ids = np.array([input_padded_tokens[\"input_ids\"]])\n",
        "\n",
        "    def next(prompt, cache, index):\n",
        "      logits = model(prompt)[:, index - 1, :]\n",
        "      hidden_states = None\n",
        "      return logits, hidden_states, cache\n",
        "\n",
        "    output_tokens = sampler(\n",
        "                    next=next,\n",
        "                    prompt=input_token_ids,\n",
        "                    index=len(np.nonzero(input_padded_tokens[\"input_ids\"])[0]) - 1)\n",
        "\n",
        "    txt = tokenizer.decode(output_tokens[0])\n",
        "\n",
        "    return txt"
      ],
      "metadata": {
        "id": "7lA8AjBF5a1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Once\"\n",
        "greedy_sampler = keras_nlp.samplers.GreedySampler()\n",
        "\n",
        "\n",
        "generated_text = generate_text(model, tokenizer, input_text, max_length=maxlen, sampler=greedy_sampler)\n",
        "print(f\"Generated Text: \\n{generated_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-AviYex5da6",
        "outputId": "531297b5-81d3-40c8-bf77-22bba2e992c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: \n",
            "[CLS] in general comedy movies the only bit the british comedies are the british comedies of the british comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies are of comedy comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies comedies\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TopK_sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
        "\n",
        "generated_text = generate_text(model, tokenizer, input_text, max_length=maxlen, sampler=TopK_sampler)\n",
        "print(f\"Generated Text: \\n{generated_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvMqWa4m5d6G",
        "outputId": "3050ecf5-7104-4e64-d0c9-dd20468eee98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: \n",
            "[CLS] in general comedy movies he is breezy romantic comedy satire the young man comedy a vice griffith the einstein is the einstein is breezy supporting role comedy director walter burns a bree bra sullivan maririshly naiveutesally einstein walter sullivan scarte sullivan is the sullivan sullivan ira hoffman flynn sullivan sullivan sullivan sullivan in romantic comedies with walter sullivan davies flynn sullivan sullivan davies flynn succeeds once a late in unexpectedly davies unexpectedly corbett while unexpectedly flynn flynn gets unexpectedly unexpectedly unexpectedly unexpectedly flynns flynn succeeds as flynn sullivan flynn is a breezy corbettly bree corbett off flynn gets mistaken flynn corbett flynn is corbett off flynn davies is the\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnU70D5DyzpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fF-Bpv48ywxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}